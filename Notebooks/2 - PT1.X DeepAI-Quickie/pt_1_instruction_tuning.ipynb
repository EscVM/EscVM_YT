{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Tuning From Scratch with a Single GPU üòã\n",
    "\n",
    "With this notebook, you can easily full fine-tune a [GPT-2 model](https://huggingface.co/gpt2) to follow instructions with a single GPU with less than 8 GB of memory!\n",
    "\n",
    "The notebook is designed to work with [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca?tab=readme-ov-file), but it is pretty easy to adapt the `get_dataloaders_alpaca` in `Notebooks/2 - PT1.X DeepAI-Quickie/utils/casual_lllm_data_utils.py` to work with a different instruction tuning dataset. Furthermore, you can scale to a larger GPT-2 model for better performances.\n",
    "\n",
    "- GPT-2: 124M parameters\n",
    "- GPT-2 medium: 355M parameters\n",
    "- GPT-2 large: 774M parameters\n",
    "- GPT-2 XL: 1.5B parameters\n",
    "\n",
    "Results with the base model are acceptable, but further hyperparameters search and tricks could most probably lead to better results. The following is the validation loss obtained with the parameters reported in this notebook. Surely, it is an ‚Äúaffordable‚Äù playground to play with this important step of the pipeline that transforms a model from an LLM to a usable and querable model.\n",
    "\n",
    "Instead, if you want to scale to a larger model, you will require to adapt the code to work with a multi-gpu environment. You can learn how to do that with [Accelerate](https://huggingface.co/docs/accelerate/index) or wait for a new notebook in this repo ;-)\n",
    "\n",
    "![ValLoss](./media/val_loss_gpt_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from utils.casual_llm_utills import generate_batch, fine_tune_llm, seed_all\n",
    "from utils.casual_lllm_data_utils import print_dataset_statistics, get_dataloaders_alpaca\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Set some variables\n",
    "MODEL_NAME = 'gpt2'\n",
    "\n",
    "DATASET_NAME = 'tatsu-lab/alpaca'\n",
    "\n",
    "# Training configs\n",
    "config_train = {\n",
    "    'num_epochs': 10,\n",
    "    'lr': 2e-5,\n",
    "    'num_warmup_steps': 300,\n",
    "    'weight_decay': 0.0,\n",
    "    'batch_size': 16,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'checkpoint_path': 'modelstore',\n",
    "    'logs_path': 'logs',\n",
    "    'max_length': 120,\n",
    "    'eval_split': 0.1,\n",
    "    'seed': 9\n",
    "}\n",
    "\n",
    "# Generation configs\n",
    "config_gen = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"do_sample\": True,\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"top_p\" : 0.92,\n",
    "    \"top_k\" : 0\n",
    "}\n",
    "\n",
    "seed_all(config_train['seed'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Import the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants special tokens\n",
    "PAD_TOKEN = '<|endoftext|>'\n",
    "BOS_TOKEN = '<|endoftext|>'\n",
    "EOS_TOKEN = '### End'\n",
    "INSTRUCTION_TOKEN = '### Instruction:'\n",
    "RESPONCE_TOKEN = '### Response:\\n'\n",
    "UKN_TOKEN = '<|endoftext|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model and tokenizer. They wil be saved in ~/.cache/huggingface\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side='left', pad_token=PAD_TOKEN)\n",
    "tokenizer.add_tokens([INSTRUCTION_TOKEN, EOS_TOKEN, RESPONCE_TOKEN])\n",
    "tokenizer.eos_token = EOS_TOKEN\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Generate text with the pre-trained mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template without ### Input:\\n\n",
    "template = \"\"\"\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = [\n",
    "    template.format(instruction='Are you alive?'),\n",
    "    template.format(instruction='What is the capital of Italy?')\n",
    "]\n",
    "\n",
    "output = generate_batch(model, tokenizer, example_text, **config_gen)\n",
    "\n",
    "for i in range(len(example_text)):\n",
    "    print(example_text[i] + output[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dataset_statistics(dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader = get_dataloaders_alpaca(dataset, tokenizer, \n",
    "                                                            eval_split=config_train['eval_split'], \n",
    "                                                            batch_size=config_train['batch_size'],\n",
    "                                                            max_length=config_train['max_length'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Fine-Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fine_tune_llm(model, tokenizer, \n",
    "                      train_dataloader, \n",
    "                      val_dataloader, **config_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = [\n",
    "    template.format(instruction='Are you alive?'),\n",
    "    template.format(instruction='What is the capital of Italy?')\n",
    "]\n",
    "\n",
    "output = generate_batch(model, tokenizer, example_text, **config_gen)\n",
    "\n",
    "for i in range(len(example_text)):\n",
    "    print(example_text[i] + output[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
